{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e7aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/workflows.py\n",
    "\n",
    "\"\"\"\n",
    "Assembles all BeeAI agents into a runnable Workflow (beeai-framework ≥ 0.1.14).\n",
    "\n",
    "Fixes & improvements\n",
    "--------------------\n",
    "* Use `wf.run(state=…)` (since this Workflow class does not support `emit`).\n",
    "* Add a robust `_find_mapping` helper: recursively inspects attributes,\n",
    "  their return values (including zero-arg callables) and items inside\n",
    "  `__dict__` until the first object satisfying `hasattr(x, \"get\")` is found.\n",
    "* Skip calling coroutine functions / coroutine objects inside `_safe_call`.\n",
    "* Keeps the public API exactly the same: `run_workflow(zip_path, …)` still\n",
    "  returns a dict with **tree_text**, **file_summaries**, **project_summary**.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import inspect\n",
    "import json\n",
    "import logging\n",
    "import types\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Set\n",
    "\n",
    "from beeai_framework.workflows.workflow import Workflow\n",
    "\n",
    "from .config import settings\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Logging\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=getattr(logging, settings.LOG_LEVEL.upper(), logging.INFO),\n",
    "    format=(\n",
    "        \"%(asctime)s | %(levelname)s | %(name)s | \"\n",
    "        \"%(module)s.%(funcName)s:%(lineno)d | ★ %(message)s\"\n",
    "    ),\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Constants & helpers\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "# Likely attribute / method names where the framework may hide its state\n",
    "LIKELY_MEM_ATTRS: tuple[str, ...] = (\n",
    "    \"state\",\n",
    "    \"_state\",\n",
    "    \"outputs\",\n",
    "    \"result\",\n",
    "    \"data\",\n",
    "    \"value\",\n",
    "    \"payload\",\n",
    "    \"memory\",\n",
    "    \"_context\",\n",
    "    \"context\",\n",
    "    \"final_state\",\n",
    "    \"state_data\",\n",
    ")\n",
    "\n",
    "# Location of the `beeai.yaml` manifest\n",
    "BEEAI_YAML = Path(__file__).parent.parent / \"beeai.yaml\"\n",
    "\n",
    "\n",
    "def _safe_call(maybe_callable: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Call `obj()` **only** if it is a synchronous, zero-argument callable.\n",
    "    Skip calling if:\n",
    "      - it's a coroutine function (async def)\n",
    "      - it returns a coroutine object (awaitable)\n",
    "      - calling it raises any exception\n",
    "    In those cases, return the object itself unmodified.\n",
    "    \"\"\"\n",
    "    # 1) Not a callable at all? Return as is.\n",
    "    if not callable(maybe_callable):\n",
    "        return maybe_callable\n",
    "\n",
    "    # 2) If it is a coroutine function (async def), do NOT call.\n",
    "    if inspect.iscoroutinefunction(maybe_callable):\n",
    "        LOG.debug(\"_safe_call: Skipping call to coroutine function %r\", maybe_callable)\n",
    "        return maybe_callable\n",
    "\n",
    "    # 3) If it is a bound method or normal function, inspect its signature.\n",
    "    sig = inspect.signature(maybe_callable)\n",
    "    # Must have no required params to call\n",
    "    for param in sig.parameters.values():\n",
    "        if (\n",
    "            param.kind not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD)\n",
    "            and param.default is inspect._empty\n",
    "        ):\n",
    "            # Found a required parameter; we cannot call it safely.\n",
    "            LOG.debug(\"_safe_call: Function %r has required params; skipping call.\", maybe_callable)\n",
    "            return maybe_callable\n",
    "\n",
    "    # 4) Attempt to call it, but guard against coroutine objects or exceptions\n",
    "    try:\n",
    "        result = maybe_callable()\n",
    "    except Exception as e:\n",
    "        LOG.warning(\"_safe_call: Calling %r raised %s; skipping.\", maybe_callable, e, exc_info=False)\n",
    "        return maybe_callable\n",
    "\n",
    "    # 5) If the result is a coroutine object (awaitable), skip it.\n",
    "    if inspect.iscoroutine(result) or isinstance(result, types.CoroutineType):\n",
    "        LOG.debug(\"_safe_call: Result of calling %r is a coroutine; returning original.\", maybe_callable)\n",
    "        return maybe_callable\n",
    "\n",
    "    # 6) Otherwise, return what we got\n",
    "    return result\n",
    "\n",
    "\n",
    "def _find_mapping_old(obj: Any, *, max_depth: int = 3) -> Mapping[str, Any] | None:\n",
    "    \"\"\"\n",
    "    Recursively search `obj` (and selected descendants) for the first dict-like\n",
    "    object (anything with a `.get` attribute).\n",
    "\n",
    "    Depth-first search; stops as soon as one mapping is found.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : Any\n",
    "        Object to inspect.\n",
    "    max_depth : int, default 3\n",
    "        Prevents infinite loops & runaway recursion.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Mapping[str, Any] | None\n",
    "        The mapping if found, else `None`.\n",
    "    \"\"\"\n",
    "    visited: Set[int] = set()\n",
    "\n",
    "    def _walk(current: Any, depth: int) -> Mapping[str, Any] | None:\n",
    "        if depth < 0 or id(current) in visited:\n",
    "            return None\n",
    "        visited.add(id(current))\n",
    "\n",
    "        # 1) If it's already a mapping, return it\n",
    "        if hasattr(current, \"get\"):\n",
    "            return current  # type: ignore[return-value]\n",
    "\n",
    "        # 2) Look at likely attributes / callables\n",
    "        for name in LIKELY_MEM_ATTRS:\n",
    "            if hasattr(current, name):\n",
    "                candidate = getattr(current, name)\n",
    "                candidate = _safe_call(candidate)\n",
    "                if hasattr(candidate, \"get\"):\n",
    "                    LOG.debug(\"_find_mapping: Found mapping via attribute %r\", name)\n",
    "                    return candidate  # type: ignore[return-value]\n",
    "\n",
    "        # 3) Inspect __dict__ values (recursively)\n",
    "        if hasattr(current, \"__dict__\"):\n",
    "            for val in current.__dict__.values():\n",
    "                candidate = _safe_call(val)\n",
    "                if hasattr(candidate, \"get\"):\n",
    "                    LOG.debug(\"_find_mapping: Found mapping inside __dict__ value %r\", val)\n",
    "                    return candidate  # type: ignore[return-value]\n",
    "                deeper = _walk(candidate, depth - 1)\n",
    "                if deeper is not None:\n",
    "                    return deeper\n",
    "\n",
    "        # 4) Nothing found in this branch\n",
    "        return None\n",
    "\n",
    "    return _walk(obj, max_depth)\n",
    "\n",
    "def _find_mapping(obj: Any, *, max_depth: int = 3) -> Mapping[str, Any] | None:\n",
    "    # 1) If it has a \"memory\" attribute that is dict-like, return it immediately.\n",
    "    if hasattr(obj, \"memory\") and isinstance(getattr(obj, \"memory\"), dict):\n",
    "        LOG.debug(\"_find_mapping: using obj.memory directly\")\n",
    "        return getattr(obj, \"memory\")\n",
    "\n",
    "    # 2) Otherwise fallback to the old DFS search (to preserve backwards compatibility)\n",
    "    def _walk(x: Any, depth: int) -> Mapping[str, Any] | None:\n",
    "        if depth <= 0:\n",
    "            return None\n",
    "\n",
    "        if hasattr(x, \"get\"):\n",
    "            return x\n",
    "\n",
    "        if hasattr(x, \"__dict__\"):\n",
    "            for val in vars(x).values():\n",
    "                if hasattr(val, \"get\"):\n",
    "                    return val\n",
    "                deeper = _walk(val, depth - 1)\n",
    "                if deeper is not None:\n",
    "                    return deeper\n",
    "\n",
    "        if hasattr(x, \"items\"):\n",
    "            for _, val in x.items():\n",
    "                if hasattr(val, \"get\"):\n",
    "                    return val\n",
    "                deeper = _walk(val, depth - 1)\n",
    "                if deeper is not None:\n",
    "                    return deeper\n",
    "\n",
    "        return None\n",
    "\n",
    "    return _walk(obj, max_depth)\n",
    "\n",
    "\n",
    "def create_workflow_engine(model: str | None = None) -> Workflow:\n",
    "    \"\"\"\n",
    "    Load a Workflow from `beeai.yaml`.\n",
    "    The *model* argument is only logged for completeness. beeai-framework ≥ 0.1.14\n",
    "    reads model settings from env / YAML directly (including OLLAMA_URL).\n",
    "    \"\"\"\n",
    "    LOG.info(\">>> Creating BeeAI Workflow engine (schema=%s, model=%s) ...\", BEEAI_YAML, model or \"default\")\n",
    "    wf = Workflow(schema=str(BEEAI_YAML))\n",
    "    LOG.info(\"✔ Workflow engine instantiated from %s\", BEEAI_YAML)\n",
    "    return wf\n",
    "\n",
    "\n",
    "# Back-compatible alias\n",
    "create_runtime = create_workflow_engine\n",
    "\n",
    "\n",
    "def run_workflow(\n",
    "    zip_path: Path,\n",
    "    *,\n",
    "    model: str | None = None,\n",
    "    print_events: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run the BeeAI multi-agent workflow on *zip_path* and return the artefacts.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any] with keys\n",
    "      • **tree_text**       – the directory tree as a string\n",
    "      • **file_summaries**  – list[dict] of per-file summaries\n",
    "      • **project_summary** – final overall summary\n",
    "    \"\"\"\n",
    "    # -----------------------------\n",
    "    # 0) Configure logging level\n",
    "    # -----------------------------\n",
    "    if print_events:\n",
    "        LOG.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        LOG.setLevel(getattr(logging, settings.LOG_LEVEL.upper(), logging.INFO))\n",
    "\n",
    "    LOG.info(\">>> Starting run_workflow for ZIP: %s\", zip_path)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) ZIP size guard-rail\n",
    "    # -----------------------------\n",
    "    max_bytes = settings.ZIP_SIZE_LIMIT_MB * 1_048_576\n",
    "    size = zip_path.stat().st_size\n",
    "    LOG.debug(\n",
    "        \"ZIP file size is %d bytes; max allowed is %d bytes (%d MB)\",\n",
    "        size,\n",
    "        max_bytes,\n",
    "        settings.ZIP_SIZE_LIMIT_MB,\n",
    "    )\n",
    "    if size > max_bytes:\n",
    "        error_msg = f\"ZIP is {size / 1_048_576:.1f} MB – exceeds limit of {settings.ZIP_SIZE_LIMIT_MB} MB\"\n",
    "        LOG.error(error_msg)\n",
    "        raise ValueError(error_msg)\n",
    "    LOG.info(\"ZIP size check passed (%0.1f MB ≤ %d MB)\", size / 1_048_576, settings.ZIP_SIZE_LIMIT_MB)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Build workflow engine\n",
    "    # -----------------------------\n",
    "    wf = create_workflow_engine(model)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Prepare initial state (since emit is unavailable)\n",
    "    # -----------------------------\n",
    "    resolved_path = str(zip_path.resolve())\n",
    "    initial_state: dict[str, Any] = {\"NewUpload\": {\"zip_path\": resolved_path}}\n",
    "    LOG.info(\"Initial state for workflow prepared: %s\", initial_state)\n",
    "\n",
    "    # If the user wants to see every event, attempt to subscribe (may be no-op)\n",
    "    if print_events:\n",
    "        try:\n",
    "            wf.subscribe(\"*\", lambda e: LOG.info(\"Event fired: %s\", e[\"type\"]))\n",
    "        except AttributeError:\n",
    "            LOG.debug(\"Workflow.subscribe is not available; skipping event logging.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Run the workflow with initial_state\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        LOG.info(\">>> Invoking wf.run(state=…) …\")\n",
    "        run_output = wf.run(state=initial_state)\n",
    "        LOG.info(\"✔ Workflow.run completed.\")\n",
    "    except Exception as e:\n",
    "        LOG.exception(\"✖ Error while running workflow: %s\", e)\n",
    "        raise\n",
    "\n",
    "    # (Optional) inspect run_output for debugging\n",
    "    LOG.debug(\"Run object returned (class=%s)\", run_output.__class__.__name__)\n",
    "    LOG.debug(\"dir(run_output) = %s\", dir(run_output))\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) Locate the memory / outputs mapping\n",
    "    # -----------------------------\n",
    "    LOG.info(\">>> Looking for BeeAI memory in the Run object …\")\n",
    "    mem = _find_mapping(run_output)\n",
    "    if mem is None:\n",
    "        LOG.error(\n",
    "            \"✖ Could not locate a dictionary-like state in the Run object. \"\n",
    "            \"Enable DEBUG logs, inspect dir(run_output), and update _find_mapping if needed.\"\n",
    "        )\n",
    "        raise AttributeError(\"No mapping attribute found in Run object.\")\n",
    "    LOG.info(\"✔ Found BeeAI memory mapping: %r\", type(mem))\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) Extract artefacts\n",
    "    # -----------------------------\n",
    "    LOG.info(\">>> Extracting 'project_tree.txt' from memory …\")\n",
    "    tree_text = mem.get(\"project_tree.txt\", \"\")\n",
    "    if tree_text:\n",
    "        LOG.debug(\n",
    "            \"First 100 chars of tree_text: %r\",\n",
    "            tree_text[:100] + (\"…\" if len(tree_text) > 100 else \"\"),\n",
    "        )\n",
    "    else:\n",
    "        LOG.warning(\"No 'project_tree.txt' found in memory.\")\n",
    "\n",
    "    LOG.info(\">>> Extracting 'file_summaries.json' from memory …\")\n",
    "    summaries_json = mem.get(\"file_summaries.json\", \"[]\")\n",
    "    if isinstance(summaries_json, str):\n",
    "        LOG.debug(\n",
    "            \"First 100 chars of summaries_json: %r\",\n",
    "            summaries_json[:100] + (\"…\" if len(summaries_json) > 100 else \"\"),\n",
    "        )\n",
    "    else:\n",
    "        LOG.debug(\"'file_summaries.json' is not a string (type=%s)\", type(summaries_json))\n",
    "\n",
    "    LOG.info(\">>> Extracting 'project_summary.txt' from memory …\")\n",
    "    project_summary = mem.get(\"project_summary.txt\", \"\")\n",
    "    if project_summary:\n",
    "        LOG.debug(\n",
    "            \"First 100 chars of project_summary: %r\",\n",
    "            project_summary[:100] + (\"…\" if len(project_summary) > 100 else \"\"),\n",
    "        )\n",
    "    else:\n",
    "        LOG.warning(\"No 'project_summary.txt' found in memory.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7) Convert summaries to list[dict]\n",
    "    # -----------------------------\n",
    "    file_summaries: List[Dict[str, Any]]\n",
    "    try:\n",
    "        if isinstance(summaries_json, str):\n",
    "            if summaries_json.strip():\n",
    "                file_summaries = json.loads(summaries_json)\n",
    "                LOG.info(\"✔ Parsed file_summaries.json into %d entries.\", len(file_summaries))\n",
    "            else:\n",
    "                LOG.warning(\"file_summaries.json was empty or whitespace.\")\n",
    "                file_summaries = []\n",
    "        else:\n",
    "            # If the framework returned a native list/dict-like structure\n",
    "            if isinstance(summaries_json, list):\n",
    "                file_summaries = summaries_json  # type: ignore[assignment]\n",
    "                LOG.info(\"✔ Retrieved file_summaries as list (length=%d).\", len(file_summaries))\n",
    "            else:\n",
    "                # Fallback: try converting to list\n",
    "                file_summaries = list(summaries_json)  # type: ignore[arg-type]\n",
    "                LOG.info(\"✔ Converted file_summaries to list (length=%d).\", len(file_summaries))\n",
    "    except Exception as exc:\n",
    "        LOG.warning(\"✖ Could not parse file_summaries.json: %s. Returning empty list.\", exc)\n",
    "        file_summaries = []\n",
    "\n",
    "    LOG.info(\"✔ Successfully processed artefacts from workflow memory.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 8) Return final dict\n",
    "    # -----------------------------\n",
    "    return {\n",
    "        \"tree_text\": tree_text,\n",
    "        \"file_summaries\": file_summaries,\n",
    "        \"project_summary\": project_summary,\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
