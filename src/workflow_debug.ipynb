{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85aa7c29",
   "metadata": {},
   "source": [
    "# Debugging Individual Agents in AI Project Analizer\n",
    "\n",
    "This notebook lets you run and debug each BeeAI agent in isolation. Each section will:\n",
    "1. Import the necessary agent class.\n",
    "2. Prepare a minimal environment (e.g., a test ZIP or test files).\n",
    "3. Call the agent's `handle()` method with simulated events.\n",
    "4. Capture and display outputs (logs, memory writes, emitted events).\n",
    "\n",
    "Make sure you have extracted the project into the folder:\n",
    "```\n",
    "/mnt/c/blog/ai-project-analizer\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff76a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_DIR: /mnt/c/blog/ai-project-analizer/backup/agent_debug_tests\n"
     ]
    }
   ],
   "source": [
    "# Setup paths and logging\n",
    "import sys, os, shutil, zipfile, tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project src and root to sys.path\n",
    "sys.path.append(r\"/mnt/c/blog/ai-project-analizer/src\")\n",
    "sys.path.append(r\"/mnt/c/blog/ai-project-analizer\")\n",
    "\n",
    "# Ensure logs are printed\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# Create a temporary directory for tests\n",
    "TEST_DIR = Path(\"/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests\")\n",
    "if TEST_DIR.exists():\n",
    "    shutil.rmtree(TEST_DIR)\n",
    "TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"TEST_DIR:\", TEST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d8313a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Event' from 'beeai_framework.agents.types' (/mnt/c/blog/ai-project-analizer/.venv/lib/python3.12/site-packages/beeai_framework/agents/types.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseAgent \u001b[38;5;28;01mas\u001b[39;00m Agent\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Event\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Event' from 'beeai_framework.agents.types' (/mnt/c/blog/ai-project-analizer/.venv/lib/python3.12/site-packages/beeai_framework/agents/types.py)"
     ]
    }
   ],
   "source": [
    "from beeai_framework.agents import BaseAgent as Agent\n",
    "from beeai_framework.agents.types import Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4a1f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\"/mnt/c/blog/ai-project-analizer\")  # adjust as needed\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0b92d",
   "metadata": {},
   "source": [
    "## Create a Sample ZIP for Testing\n",
    "\n",
    "We'll generate a small ZIP file containing a single `foo.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0b77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created zip at: /mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip\n"
     ]
    }
   ],
   "source": [
    "# Create a simple zip file with one text file\n",
    "sample_zip_path = TEST_DIR / \"test.zip\"\n",
    "with zipfile.ZipFile(sample_zip_path, \"w\") as zf:\n",
    "    # Create a text file inside a temp location\n",
    "    tmp_file = TEST_DIR / \"foo.txt\"\n",
    "    tmp_file.write_text(\"Hello, world!\")\n",
    "    zf.write(tmp_file, arcname=\"foo.txt\")\n",
    "print(\"Created zip at:\", sample_zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0001030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created zip at: /mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip\n",
      "Exists on disk? True Size: 125\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "# 1) Make sure TEST_DIR and sample_zip_path exist\n",
    "#    (you presumably already have TEST_DIR from earlier cells).\n",
    "sample_zip_path = TEST_DIR / \"test.zip\"\n",
    "\n",
    "# 2) Create the ZIP _before_ you hand it to the agent:\n",
    "with zipfile.ZipFile(sample_zip_path, \"w\") as zf:\n",
    "    tmp_file = TEST_DIR / \"foo.txt\"\n",
    "    tmp_file.write_text(\"Hello, world!\")\n",
    "    zf.write(tmp_file, arcname=\"foo.txt\")\n",
    "\n",
    "print(\"Created zip at:\", sample_zip_path)\n",
    "print(\"Exists on disk?\", sample_zip_path.exists(), \"Size:\", sample_zip_path.stat().st_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9b54135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:20:10,500 INFO [config] Loading settings from default environment or .env\n",
      "2025-06-01 10:20:10,500 DEBUG [config] Final settings: {'BEEAI_MODEL': 'openai/gpt-4o-mini', 'DELETE_TEMP_AFTER_RUN': True, 'ZIP_SIZE_LIMIT_MB': 300, 'MAX_MEMBER_SIZE_MB': 150, 'LOG_LEVEL': 'INFO'}\n",
      "2025-06-01 10:20:10,501 INFO >>> [zip_validator] handle() entered. event={'type': 'NewUpload', 'zip_path': '/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip'}\n",
      "2025-06-01 10:20:10,501 INFO [zip_validator] Received NewUpload for PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip')\n",
      "2025-06-01 10:20:10,505 DEBUG [zip_validator] Checking size: 125 bytes (max 314572800 bytes)\n",
      "2025-06-01 10:20:10,505 DEBUG [zip_validator] Checking if PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip') is a zipfile\n",
      "2025-06-01 10:20:10,507 DEBUG [zip_validator] Performing CRC check on PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip')\n",
      "2025-06-01 10:20:10,509 INFO [zip_validator] ZIP archive PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip') passed all checks\n",
      "2025-06-01 10:20:10,510 INFO [zip_validator] Emitted ZipValid for PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip')\n",
      "2025-06-01 10:20:10,510 INFO [zip_validator] handle() exiting normally.\n",
      "2025-06-01 10:20:10,510 INFO >>> [zip_validator] handle() entered. event={'type': 'NewUpload', 'zip_path': '/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/nonexistent.zip'}\n",
      "2025-06-01 10:20:10,511 INFO [zip_validator] Received NewUpload for PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/nonexistent.zip')\n",
      "2025-06-01 10:20:10,512 WARNING [zip_validator] File does not exist: /mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/nonexistent.zip\n",
      "2025-06-01 10:20:10,512 INFO [zip_validator] Emitted ZipInvalid (file missing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emitted events for valid ZIP: [('ZipValid', {'zip_path': '/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip'})]\n",
      "Emitted events for bad ZIP: [('ZipInvalid', {'zip_path': '/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/nonexistent.zip', 'reason': 'File does not exist'})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Now import and test ZipValidatorAgent\n",
    "from src.agents.zip_validator_agent import ZipValidatorAgent\n",
    "\n",
    "zip_agent = ZipValidatorAgent()\n",
    "emitted = []\n",
    "zip_agent.emit = lambda event_name, payload: emitted.append((event_name, payload))\n",
    "\n",
    "# Test with the *existing* ZIP:\n",
    "event = {\"type\": \"NewUpload\", \"zip_path\": str(sample_zip_path)}\n",
    "zip_agent.handle(event)\n",
    "print(\"Emitted events for valid ZIP:\", emitted)\n",
    "\n",
    "# Test with a truly missing path:\n",
    "emitted.clear()\n",
    "event_bad = {\"type\": \"NewUpload\", \"zip_path\": str(TEST_DIR / \"nonexistent.zip\")}\n",
    "zip_agent.handle(event_bad)\n",
    "print(\"Emitted events for bad ZIP:\", emitted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa767d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:20:15,957 INFO >>> [zip_validator] handle() entered. event={'type': 'NewUpload', 'zip_path': '/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip'}\n",
      "2025-06-01 10:20:15,957 INFO [zip_validator] Received NewUpload for PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip')\n",
      "2025-06-01 10:20:15,962 DEBUG [zip_validator] Checking size: 125 bytes (max 314572800 bytes)\n",
      "2025-06-01 10:20:15,963 DEBUG [zip_validator] Checking if PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip') is a zipfile\n",
      "2025-06-01 10:20:15,966 DEBUG [zip_validator] Performing CRC check on PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip')\n",
      "2025-06-01 10:20:15,969 INFO [zip_validator] ZIP archive PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip') passed all checks\n",
      "2025-06-01 10:20:15,969 INFO [zip_validator] Emitted ZipValid for PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip')\n",
      "2025-06-01 10:20:15,970 INFO [zip_validator] handle() exiting normally.\n",
      "2025-06-01 10:20:15,970 INFO >>> [zip_validator] handle() entered. event={'type': 'NewUpload', 'zip_path': '/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/nonexistent.zip'}\n",
      "2025-06-01 10:20:15,970 INFO [zip_validator] Received NewUpload for PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/nonexistent.zip')\n",
      "2025-06-01 10:20:15,972 WARNING [zip_validator] File does not exist: /mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/nonexistent.zip\n",
      "2025-06-01 10:20:15,972 INFO [zip_validator] Emitted ZipInvalid (file missing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emitted events for valid ZIP: [('ZipValid', {'zip_path': '/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip'})]\n",
      "Emitted events for bad ZIP: [('ZipInvalid', {'zip_path': '/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/nonexistent.zip', 'reason': 'File does not exist'})]\n"
     ]
    }
   ],
   "source": [
    "from src.agents.zip_validator_agent import ZipValidatorAgent\n",
    "\n",
    "# Prepare the agent\n",
    "zip_agent = ZipValidatorAgent()\n",
    "emitted = []\n",
    "# Monkey‐patch emit to capture events\n",
    "zip_agent.emit = lambda event_name, payload: emitted.append((event_name, payload))\n",
    "\n",
    "# Test with valid ZIP (assuming sample_zip_path is defined)\n",
    "event = {\"type\": \"NewUpload\", \"zip_path\": str(sample_zip_path)}\n",
    "zip_agent.handle(event)\n",
    "print(\"Emitted events for valid ZIP:\", emitted)\n",
    "\n",
    "# Test with an invalid path\n",
    "emitted.clear()\n",
    "event_bad = {\"type\": \"NewUpload\", \"zip_path\": str(TEST_DIR / \"nonexistent.zip\")}\n",
    "zip_agent.handle(event_bad)\n",
    "print(\"Emitted events for bad ZIP:\", emitted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e1b04",
   "metadata": {},
   "source": [
    "## Debug `ZipValidatorAgent`\n",
    "\n",
    "Instantiate `ZipValidatorAgent`, monkey-patch its `emit` to capture events, and run with our test ZIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c423345",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'beeai_framework.agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mzip_validator_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ZipValidatorAgent\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Prepare the agent\u001b[39;00m\n\u001b[32m      4\u001b[39m zip_agent = ZipValidatorAgent()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/blog/ai-project-analizer/src/agents/zip_validator_agent.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Updated imports to use beeai_framework instead of beeai\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Event\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m settings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'beeai_framework.agent'"
     ]
    }
   ],
   "source": [
    "from src.agents.zip_validator_agent import ZipValidatorAgent\n",
    "\n",
    "# Prepare the agent\n",
    "zip_agent = ZipValidatorAgent()\n",
    "emitted = []\n",
    "# Monkey-patch emit to capture events\n",
    "zip_agent.emit = lambda event_name, payload: emitted.append((event_name, payload))\n",
    "\n",
    "# Test with valid ZIP\n",
    "event = {\"type\": \"NewUpload\", \"zip_path\": str(sample_zip_path)}\n",
    "zip_agent.handle(event)\n",
    "print(\"Emitted events for valid ZIP:\", emitted)\n",
    "\n",
    "# Test with an invalid path\n",
    "emitted.clear()\n",
    "event_bad = {\"type\": \"NewUpload\", \"zip_path\": str(TEST_DIR / \"nonexistent.zip\")}\n",
    "zip_agent.handle(event_bad)\n",
    "print(\"Emitted events for bad ZIP:\", emitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945f42f",
   "metadata": {},
   "source": [
    "## Debug `ExtractionAgent`\n",
    "\n",
    "Use the valid ZIP event and capture `FileDiscovered` and `ExtractionDone`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b00020f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'beeai_framework.agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextraction_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExtractionAgent\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Prepare the agent\u001b[39;00m\n\u001b[32m      4\u001b[39m ext_agent = ExtractionAgent()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/blog/ai-project-analizer/src/agents/extraction_agent.py:30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Updated imports to use beeai_framework instead of beeai\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Event\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m settings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'beeai_framework.agent'"
     ]
    }
   ],
   "source": [
    "from src.agents.extraction_agent import ExtractionAgent\n",
    "\n",
    "# Prepare the agent\n",
    "ext_agent = ExtractionAgent()\n",
    "emitted_ext = []\n",
    "ext_agent.emit = lambda event_name, payload: emitted_ext.append((event_name, payload))\n",
    "\n",
    "# Run extraction on the valid ZIP event\n",
    "event_zip_valid = {\"type\": \"ZipValid\", \"zip_path\": str(sample_zip_path)}\n",
    "ext_agent.handle(event_zip_valid)\n",
    "print(\"Extraction emitted events:\")\n",
    "for evt in emitted_ext:\n",
    "    print(evt)\n",
    "\n",
    "# Check extracted directory\n",
    "if emitted_ext and emitted_ext[-1][0] == \"ExtractionDone\":\n",
    "    base_dir = Path(emitted_ext[-1][1][\"base_dir\"])\n",
    "    print(\"Contents of extracted directory:\", list(base_dir.rglob(\"*\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "690a8bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:20:23,409 INFO >>> [extraction] handle() entered. event={'type': 'ZipValid', 'zip_path': '/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip'}\n",
      "2025-06-01 10:20:23,409 INFO [extraction] Received ZipValid for PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip')\n",
      "2025-06-01 10:20:23,409 DEBUG [extraction] Calling safe_extract for PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip')\n",
      "2025-06-01 10:20:23,410 INFO [extraction] safe_extract() called with PosixPath('/mnt/c/blog/ai-project-analizer/backup/agent_debug_tests/test.zip')\n",
      "2025-06-01 10:20:23,410 INFO [extraction] Created temporary directory PosixPath('/tmp/ai_analyser_4eae4w6d')\n",
      "2025-06-01 10:20:23,412 DEBUG [extraction] Inspecting member 'foo.txt' (size=27 bytes)\n",
      "2025-06-01 10:20:23,412 INFO [extraction] All members passed size and Zip‐Slip checks, extracting now...\n",
      "2025-06-01 10:20:23,413 INFO [extraction] Extraction completed into PosixPath('/tmp/ai_analyser_4eae4w6d')\n",
      "2025-06-01 10:20:23,413 INFO [extraction] safe_extract returned PosixPath('/tmp/ai_analyser_4eae4w6d')\n",
      "2025-06-01 10:20:23,413 INFO [extraction] Walking directory PosixPath('/tmp/ai_analyser_4eae4w6d') to discover files\n",
      "2025-06-01 10:20:23,414 DEBUG [extraction] Discovered file PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:20:23,414 INFO [extraction] Emitted FileDiscovered for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:20:23,414 INFO [extraction] Full list of discovered files (total=1):\n",
      "2025-06-01 10:20:23,415 INFO    - PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:20:23,415 INFO [extraction] All files discovered. Emitting ExtractionDone with base_dir=PosixPath('/tmp/ai_analyser_4eae4w6d')\n",
      "2025-06-01 10:20:23,415 INFO [extraction] Emitted ExtractionDone. handle() exiting normally.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP exists:  True\n",
      "\n",
      "Extraction emitted events:\n",
      "('FileDiscovered', {'path': '/tmp/ai_analyser_4eae4w6d/foo.txt'})\n",
      "('ExtractionDone', {'base_dir': '/tmp/ai_analyser_4eae4w6d'})\n",
      "\n",
      "Contents of extracted directory:\n",
      "   /tmp/ai_analyser_4eae4w6d/foo.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "# (1) Create the test ZIP if you haven't already)\n",
    "sample_zip_path = TEST_DIR / \"test.zip\"\n",
    "with zipfile.ZipFile(sample_zip_path, \"w\") as zf:\n",
    "    tmp = TEST_DIR / \"foo.txt\"\n",
    "    tmp.write_text(\"Hello from extraction test!\")\n",
    "    zf.write(tmp, arcname=\"foo.txt\")\n",
    "\n",
    "print(\"ZIP exists: \", sample_zip_path.exists())\n",
    "\n",
    "# (2) Now import and test ExtractionAgent:\n",
    "from src.agents.extraction_agent import ExtractionAgent\n",
    "\n",
    "ext_agent = ExtractionAgent()\n",
    "emitted_ext = []\n",
    "# Monkey-patch emit to capture events\n",
    "ext_agent.emit = lambda event_name, payload: emitted_ext.append((event_name, payload))\n",
    "\n",
    "# (3) Send the ZipValid event:\n",
    "event_zip_valid = {\"type\": \"ZipValid\", \"zip_path\": str(sample_zip_path)}\n",
    "ext_agent.handle(event_zip_valid)\n",
    "\n",
    "print(\"\\nExtraction emitted events:\")\n",
    "for evt in emitted_ext:\n",
    "    print(evt)\n",
    "\n",
    "# (4) If extraction succeeded, the last event should be ExtractionDone:\n",
    "if emitted_ext and emitted_ext[-1][0] == \"ExtractionDone\":\n",
    "    base_dir = Path(emitted_ext[-1][1][\"base_dir\"])\n",
    "    print(\"\\nContents of extracted directory:\")\n",
    "    for p in sorted(base_dir.rglob(\"*\")):\n",
    "        print(\"  \", p)\n",
    "else:\n",
    "    print(\"\\nNo ExtractionDone event; maybe ExtractionFailed was emitted above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7c58a",
   "metadata": {},
   "source": [
    "## Debug `TreeBuilderAgent`\n",
    "\n",
    "Simulate `FileDiscovered` events, then `ExtractionDone` with the previously extracted folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be7d9c86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'beeai_framework.agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_builder_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TreeBuilderAgent\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Find the base_dir from previous extraction\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m emitted_ext \u001b[38;5;129;01mand\u001b[39;00m emitted_ext[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mExtractionDone\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/blog/ai-project-analizer/src/agents/tree_builder_agent.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Updated imports to use beeai_framework instead of beeai\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Event\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'beeai_framework.agent'"
     ]
    }
   ],
   "source": [
    "from src.agents.tree_builder_agent import TreeBuilderAgent\n",
    "\n",
    "# Find the base_dir from previous extraction\n",
    "if emitted_ext and emitted_ext[-1][0] == \"ExtractionDone\":\n",
    "    base_dir = Path(emitted_ext[-1][1][\"base_dir\"])\n",
    "else:\n",
    "    raise RuntimeError(\"Extraction did not complete successfully.\")\n",
    "\n",
    "# Prepare tree builder\n",
    "tree_agent = TreeBuilderAgent()\n",
    "# Send FileDiscovered events (for each file in base_dir)\n",
    "for file_path in base_dir.rglob(\"*\"):\n",
    "    if file_path.is_file():\n",
    "        file_event = {\"type\": \"FileDiscovered\", \"path\": str(file_path)}\n",
    "        tree_agent.handle(file_event)\n",
    "\n",
    "# Now fire ExtractionDone\n",
    "done_event = {\"type\": \"ExtractionDone\", \"base_dir\": str(base_dir)}\n",
    "tree_agent.handle(done_event)\n",
    "\n",
    "# The tree text is stored in memory\n",
    "print(\"Generated project_tree.txt:\")\n",
    "print(tree_agent.memory.get(\"project_tree.txt\", \"<none>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb6d57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:20:28,542 INFO [tree_builder] Initialized with empty paths list and no base_dir\n",
      "2025-06-01 10:20:28,543 INFO >>> [tree_builder] handle() entered. event={'type': 'FileDiscovered', 'path': '/tmp/ai_analyser_4eae4w6d/foo.txt'}\n",
      "2025-06-01 10:20:28,543 DEBUG [tree_builder] Received FileDiscovered for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:20:28,544 INFO [tree_builder] Added PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt') to paths (total now: 1)\n",
      "2025-06-01 10:20:28,544 INFO >>> [tree_builder] handle() entered. event={'type': 'ExtractionDone', 'base_dir': '/tmp/ai_analyser_4eae4w6d'}\n",
      "2025-06-01 10:20:28,544 INFO [tree_builder] Received ExtractionDone, base_dir set to PosixPath('/tmp/ai_analyser_4eae4w6d')\n",
      "2025-06-01 10:20:28,545 INFO [tree_builder] _build_and_emit_tree() called with base_dir=PosixPath('/tmp/ai_analyser_4eae4w6d') and 1 paths\n",
      "2025-06-01 10:20:28,545 DEBUG [tree_builder] Using rich to build tree\n",
      "2025-06-01 10:20:28,546 DEBUG [tree_builder] Added node 'foo.txt' under parent ('ai_analyser_4eae4w6d',)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ai_analyser_4eae4w6d/</span>\n",
       "└── foo.txt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mai_analyser_4eae4w6d/\u001b[0m\n",
       "└── foo.txt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:20:28,548 INFO [tree_builder] Rich tree built successfully (length=34 chars)\n",
      "2025-06-01 10:20:28,548 INFO [tree_builder] Stored 'project_tree.txt' in memory (length=34)\n",
      "2025-06-01 10:20:28,549 INFO [tree_builder] Emitted TreeBuilt with path 'project_tree.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emitted events from TreeBuilderAgent:\n",
      "[('TreeBuilt', {'tree_path': 'project_tree.txt'})]\n",
      "\n",
      "Generated project_tree.txt in memory:\n",
      "ai_analyser_4eae4w6d/\n",
      "└── foo.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# (1) Ensure “emitted_ext” contains an ExtractionDone event, e.g.:\n",
    "#     emitted_ext = [('FileDiscovered', {...}), ('ExtractionDone', {'base_dir': '/tmp/…'})]\n",
    "\n",
    "if emitted_ext and emitted_ext[-1][0] == \"ExtractionDone\":\n",
    "    base_dir = Path(emitted_ext[-1][1][\"base_dir\"])\n",
    "else:\n",
    "    raise RuntimeError(\"Extraction did not complete successfully.\")\n",
    "\n",
    "# (2) Prepare TreeBuilderAgent and capture its emits\n",
    "from src.agents.tree_builder_agent import TreeBuilderAgent\n",
    "\n",
    "tree_agent = TreeBuilderAgent()\n",
    "emitted_tree = []\n",
    "tree_agent.emit = lambda event_name, payload: emitted_tree.append((event_name, payload))\n",
    "\n",
    "# (3) Send FileDiscovered events for each file under base_dir\n",
    "for file_path in base_dir.rglob(\"*\"):\n",
    "    if file_path.is_file():\n",
    "        file_event = {\"type\": \"FileDiscovered\", \"path\": str(file_path)}\n",
    "        tree_agent.handle(file_event)\n",
    "\n",
    "# (4) Fire ExtractionDone\n",
    "done_event = {\"type\": \"ExtractionDone\", \"base_dir\": str(base_dir)}\n",
    "tree_agent.handle(done_event)\n",
    "\n",
    "# (5) Check emitted events and memory\n",
    "print(\"Emitted events from TreeBuilderAgent:\")\n",
    "print(emitted_tree)  # Expect [('TreeBuilt', {'tree_path': 'project_tree.txt'})]\n",
    "\n",
    "print(\"\\nGenerated project_tree.txt in memory:\")\n",
    "print(tree_agent.memory.get(\"project_tree.txt\", \"<none>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ca56e",
   "metadata": {},
   "source": [
    "## Debug `FileTriageAgent`\n",
    "\n",
    "Queue discovered files, flush on `ExtractionDone`, and capture `FileForAnalysis` and `TriageComplete`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2680100",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'beeai_framework.agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_triage_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileTriageAgent\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Prepare file triage agent\u001b[39;00m\n\u001b[32m      4\u001b[39m triage_agent = FileTriageAgent()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/blog/ai-project-analizer/src/agents/file_triage_agent.py:31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Tuple\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Updated imports to use beeai_framework instead of beeai\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Event\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_io_tool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m looks_binary, priority_score, ASSET_SKIP_EXTS\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'beeai_framework.agent'"
     ]
    }
   ],
   "source": [
    "from src.agents.file_triage_agent import FileTriageAgent\n",
    "\n",
    "# Prepare file triage agent\n",
    "triage_agent = FileTriageAgent()\n",
    "emitted_triage = []\n",
    "triage_agent.emit = lambda event_name, payload: emitted_triage.append((event_name, payload))\n",
    "\n",
    "# Send FileDiscovered events for all files under base_dir\n",
    "for file_path in base_dir.rglob(\"*\"):\n",
    "    if file_path.is_file():\n",
    "        file_event = {\"type\": \"FileDiscovered\", \"path\": str(file_path)}\n",
    "        triage_agent.handle(file_event)\n",
    "\n",
    "# Now send ExtractionDone\n",
    "done_event = {\"type\": \"ExtractionDone\", \"base_dir\": str(base_dir)}\n",
    "triage_agent.handle(done_event)\n",
    "\n",
    "print(\"Triage emitted events:\")\n",
    "for evt in emitted_triage:\n",
    "    print(evt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0375a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:20:36,957 INFO [file_triage] Initialized with empty queue and extraction_done=False\n",
      "2025-06-01 10:20:36,958 INFO >>> [file_triage] handle() entered. event={'type': 'FileDiscovered', 'path': '/tmp/ai_analyser_4eae4w6d/foo.txt'}\n",
      "2025-06-01 10:20:36,959 DEBUG [file_triage] Received FileDiscovered for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:20:36,959 DEBUG [file_triage] _handle_file() called with PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:20:36,960 DEBUG [file_io_tool] looks_binary() called for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt') (sample=1024 bytes)\n",
      "2025-06-01 10:20:36,960 DEBUG [file_io_tool] File PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt'): non_printable=0, total=27, ratio=0.00\n",
      "2025-06-01 10:20:36,961 INFO [file_io_tool] looks_binary() returns False for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:20:36,961 DEBUG [file_io_tool] priority_score() called for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:20:36,962 INFO [file_io_tool] priority_score for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt') = 70 (documentation)\n",
      "2025-06-01 10:20:36,963 INFO [file_triage] Queuing file PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt') with priority score 70\n",
      "2025-06-01 10:20:36,963 INFO >>> [file_triage] handle() entered. event={'type': 'ExtractionDone', 'base_dir': '/tmp/ai_analyser_4eae4w6d'}\n",
      "2025-06-01 10:20:36,963 INFO [file_triage] Received ExtractionDone with base_dir='/tmp/ai_analyser_4eae4w6d'\n",
      "2025-06-01 10:20:36,964 INFO [file_triage] _flush_queue() called, queue length=1\n",
      "2025-06-01 10:20:36,964 INFO [file_triage] Emitting FileForAnalysis for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt') (score=70)\n",
      "2025-06-01 10:20:36,964 DEBUG [file_io_tool] priority_score() called for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:20:36,965 INFO [file_io_tool] priority_score for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt') = 70 (documentation)\n",
      "2025-06-01 10:20:36,965 DEBUG [file_triage] Emitted FileForAnalysis event for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:20:36,965 INFO [file_triage] Emitting TriageComplete\n",
      "2025-06-01 10:20:36,965 DEBUG [file_triage] Emitted TriageComplete, handle() exiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triage emitted events:\n",
      "('FileForAnalysis', {'path': '/tmp/ai_analyser_4eae4w6d/foo.txt', 'score': 70})\n",
      "('TriageComplete', {})\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# (1) Make sure `base_dir` came from ExtractionDone; e.g.:\n",
    "#     emitted_ext = [('FileDiscovered', {...}), ('ExtractionDone', {'base_dir': '/tmp/…'})]\n",
    "\n",
    "if emitted_ext and emitted_ext[-1][0] == \"ExtractionDone\":\n",
    "    base_dir = Path(emitted_ext[-1][1][\"base_dir\"])\n",
    "else:\n",
    "    raise RuntimeError(\"Extraction did not complete successfully.\")\n",
    "\n",
    "# (2) Prepare FileTriageAgent and capture emits\n",
    "from src.agents.file_triage_agent import FileTriageAgent\n",
    "\n",
    "triage_agent = FileTriageAgent()\n",
    "emitted_triage = []\n",
    "triage_agent.emit = lambda event_name, payload: emitted_triage.append((event_name, payload))\n",
    "\n",
    "# (3) Send FileDiscovered for each file under base_dir\n",
    "for file_path in base_dir.rglob(\"*\"):\n",
    "    if file_path.is_file():\n",
    "        file_event = {\"type\": \"FileDiscovered\", \"path\": str(file_path)}\n",
    "        triage_agent.handle(file_event)\n",
    "\n",
    "# (4) Send ExtractionDone\n",
    "done_event = {\"type\": \"ExtractionDone\", \"base_dir\": str(base_dir)}\n",
    "triage_agent.handle(done_event)\n",
    "\n",
    "# (5) Inspect what was emitted\n",
    "print(\"Triage emitted events:\")\n",
    "for evt in emitted_triage:\n",
    "    print(evt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f2852",
   "metadata": {},
   "source": [
    "## Debug `FileAnalysisAgent`\n",
    "\n",
    "Analyze each file from `FileForAnalysis` events and finalize on `TriageComplete`. Display `file_summaries.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e2d2574",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'beeai_framework.agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_analysis_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileAnalysisAgent\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Prepare file analysis agent\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/blog/ai-project-analizer/src/agents/file_analysis_agent.py:30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Updated imports to use beeai_framework instead of beeai\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Event\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'beeai_framework.agent'"
     ]
    }
   ],
   "source": [
    "from src.agents.file_analysis_agent import FileAnalysisAgent\n",
    "import json\n",
    "\n",
    "# Prepare file analysis agent\n",
    "analysis_agent = FileAnalysisAgent()\n",
    "emitted_analysis = []\n",
    "analysis_agent.emit = lambda event_name, payload: emitted_analysis.append((event_name, payload))\n",
    "\n",
    "# Simulate FileForAnalysis events from triage\n",
    "file_for_analysis_events = [evt for evt in emitted_triage if evt[0] == \"FileForAnalysis\"]\n",
    "for _, payload in file_for_analysis_events:\n",
    "    evt = {\"type\": \"FileForAnalysis\", \"path\": payload[\"path\"], \"score\": payload.get(\"score\", 0)}\n",
    "    analysis_agent.handle(evt)\n",
    "\n",
    "# Now fire TriageComplete\n",
    "tc_event = {\"type\": \"TriageComplete\"}\n",
    "analysis_agent.handle(tc_event)\n",
    "\n",
    "print(\"FileAnalysis emitted events:\")\n",
    "for evt in emitted_analysis:\n",
    "    print(evt)\n",
    "\n",
    "# Check memory for file_summaries.json\n",
    "print(\"Memory file_summaries.json:\")\n",
    "print(analysis_agent.memory.get(\"file_summaries.json\", \"<none>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c9e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:22:59,488 INFO >>> [file_analysis] handle() entered. event={'type': 'FileForAnalysis', 'path': '/tmp/ai_analyser_4eae4w6d/foo.txt', 'score': 70}\n",
      "2025-06-01 10:22:59,488 INFO [file_analysis] Received FileForAnalysis for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:22:59,488 DEBUG [file_analysis] _analyse() called with path=PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt'), base=PosixPath('/tmp/ai_analyser_4eae4w6d')\n",
      "2025-06-01 10:22:59,489 DEBUG [file_analysis] analyse_file() called for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt'), base=PosixPath('/tmp/ai_analyser_4eae4w6d')\n",
      "2025-06-01 10:22:59,489 DEBUG [file_analysis] Read 27 characters from PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:22:59,490 INFO [file_analysis] Fallback text summary for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt'): 'Hello from extraction test!'\n",
      "2025-06-01 10:22:59,490 INFO [file_analysis] Analysis result for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt'): {'rel_path': 'foo.txt', 'kind': 'text', 'summary': 'Hello from extraction test!'}\n",
      "2025-06-01 10:22:59,490 DEBUG [file_analysis] Appended result, total results count=1\n",
      "2025-06-01 10:22:59,491 INFO [file_analysis] Emitted FileAnalysed for PosixPath('/tmp/ai_analyser_4eae4w6d/foo.txt')\n",
      "2025-06-01 10:22:59,491 INFO >>> [file_analysis] handle() entered. event={'type': 'ExtractionDone', 'base_dir': '/tmp/ai_analyser_4eae4w6d'}\n",
      "2025-06-01 10:22:59,492 INFO [file_analysis] Received ExtractionDone, base_dir set to PosixPath('/tmp/ai_analyser_4eae4w6d')\n",
      "2025-06-01 10:22:59,492 INFO >>> [file_analysis] handle() entered. event={'type': 'TriageComplete'}\n",
      "2025-06-01 10:22:59,492 INFO [file_analysis] Received TriageComplete, finalising analysis\n",
      "2025-06-01 10:22:59,492 INFO [file_analysis] _finalise() called, total files analysed=1\n",
      "2025-06-01 10:22:59,493 INFO [file_analysis] Stored 'file_summaries.json' in memory (103 bytes)\n",
      "2025-06-01 10:22:59,493 INFO [file_analysis] Emitted AnalysisComplete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileAnalysis emitted events:\n",
      "('FileAnalysed', {'rel_path': 'foo.txt', 'kind': 'text', 'summary': 'Hello from extraction test!'})\n",
      "('AnalysisComplete', {})\n",
      "\n",
      "Memory file_summaries.json:\n",
      "[\n",
      "  {\n",
      "    \"rel_path\": \"foo.txt\",\n",
      "    \"kind\": \"text\",\n",
      "    \"summary\": \"Hello from extraction test!\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from src.agents.file_analysis_agent import FileAnalysisAgent\n",
    "import json\n",
    "\n",
    "# (1) Prepare the agent and capture emits\n",
    "analysis_agent = FileAnalysisAgent()\n",
    "emitted_analysis = []\n",
    "analysis_agent.emit = lambda event_name, payload: emitted_analysis.append((event_name, payload))\n",
    "\n",
    "# (2) Simulate FileForAnalysis events from triage\n",
    "file_for_analysis_events = [evt for evt in emitted_triage if evt[0] == \"FileForAnalysis\"]\n",
    "for _, payload in file_for_analysis_events:\n",
    "    evt = {\"type\": \"FileForAnalysis\", \"path\": payload[\"path\"], \"score\": payload.get(\"score\", 0)}\n",
    "    analysis_agent.handle(evt)\n",
    "\n",
    "# (3) Simulate ExtractionDone (so that base_dir is set if needed)\n",
    "#     If you want the agent to know base_dir, do:\n",
    "if emitted_ext and emitted_ext[-1][0] == \"ExtractionDone\":\n",
    "    base_dir_evt = {\"type\": \"ExtractionDone\", \"base_dir\": emitted_ext[-1][1][\"base_dir\"]}\n",
    "    analysis_agent.handle(base_dir_evt)\n",
    "\n",
    "# (4) Now fire TriageComplete\n",
    "tc_event = {\"type\": \"TriageComplete\"}\n",
    "analysis_agent.handle(tc_event)\n",
    "\n",
    "# (5) Inspect emitted events and memory\n",
    "print(\"FileAnalysis emitted events:\")\n",
    "for evt in emitted_analysis:\n",
    "    print(evt)\n",
    "\n",
    "print(\"\\nMemory file_summaries.json:\")\n",
    "print(analysis_agent.memory.get(\"file_summaries.json\", \"<none>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbd284",
   "metadata": {},
   "source": [
    "## Debug `SummarySynthesizerAgent`\n",
    "\n",
    "Feed `TreeBuilt` and `FileAnalysed` / `AnalysisComplete` events, then inspect `project_summary.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f8bfaad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'beeai_framework.agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msummary_synthesizer_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SummarySynthesizerAgent\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Prepare summary synthesizer agent\u001b[39;00m\n\u001b[32m      4\u001b[39m summary_agent = SummarySynthesizerAgent()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/blog/ai-project-analizer/src/agents/summary_synthesizer_agent.py:44\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Updated imports to use beeai_framework instead of beeai\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbeeai_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Event\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m settings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'beeai_framework.agent'"
     ]
    }
   ],
   "source": [
    "from src.agents.summary_synthesizer_agent import SummarySynthesizerAgent\n",
    "\n",
    "# Prepare summary synthesizer agent\n",
    "summary_agent = SummarySynthesizerAgent()\n",
    "emitted_summary = []\n",
    "summary_agent.emit = lambda event_name, payload: emitted_summary.append((event_name, payload))\n",
    "\n",
    "# Manually populate memory for tree text and file summaries\n",
    "summary_agent.memory[\"project_tree.txt\"] = tree_agent.memory.get(\"project_tree.txt\", \"\")\n",
    "# Use the list of analyses from emitted_analysis if any; else simulate one\n",
    "analyses = [payload for (name, payload) in emitted_analysis if name == \"FileAnalysed\"]\n",
    "for analysis in analyses:\n",
    "    evt = {\"type\": \"FileAnalysed\", **analysis}\n",
    "    summary_agent.handle(evt)\n",
    "\n",
    "# Now signal AnalysisComplete\n",
    "ac_event = {\"type\": \"AnalysisComplete\"}\n",
    "summary_agent.handle(ac_event)\n",
    "\n",
    "# Also signal TreeBuilt (to ensure it has the tree)\n",
    "tb_event = {\"type\": \"TreeBuilt\", \"tree_path\": \"project_tree.txt\"}\n",
    "summary_agent.handle(tb_event)\n",
    "\n",
    "print(\"SummarySynthesizer emitted events:\")\n",
    "for evt in emitted_summary:\n",
    "    print(evt)\n",
    "\n",
    "# Check memory for project_summary.txt\n",
    "print(\"\\nGenerated project_summary.txt:\")\n",
    "print(summary_agent.memory.get(\"project_summary.txt\", \"<none>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d01fee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:25:18,813 INFO [summary_synthesizer] Initialized with model='openai/gpt-4o-mini'\n",
      "2025-06-01 10:25:18,813 INFO >>> [summary_synthesizer] handle() entered. event={'type': 'FileAnalysed', 'rel_path': 'foo.txt', 'kind': 'text', 'summary': 'Hello from extraction test!'}\n",
      "2025-06-01 10:25:18,814 DEBUG [summary_synthesizer] Received FileAnalysed: {'type': 'FileAnalysed', 'rel_path': 'foo.txt', 'kind': 'text', 'summary': 'Hello from extraction test!'}\n",
      "2025-06-01 10:25:18,814 INFO [summary_synthesizer] Appended analysis for 'foo.txt', total=1\n",
      "2025-06-01 10:25:18,814 INFO >>> [summary_synthesizer] handle() entered. event={'type': 'AnalysisComplete'}\n",
      "2025-06-01 10:25:18,814 INFO [summary_synthesizer] Received AnalysisComplete event\n",
      "2025-06-01 10:25:18,815 INFO [summary_synthesizer] _maybe_finish() called. analysis_done=True, tree_text_set=False\n",
      "2025-06-01 10:25:18,815 DEBUG [summary_synthesizer] Waiting for both tree and analyses\n",
      "2025-06-01 10:25:18,815 INFO >>> [summary_synthesizer] handle() entered. event={'type': 'TreeBuilt', 'tree_path': 'project_tree.txt'}\n",
      "2025-06-01 10:25:18,816 INFO [summary_synthesizer] Received TreeBuilt, tree_path='project_tree.txt'\n",
      "2025-06-01 10:25:18,816 DEBUG [summary_synthesizer] Loaded tree text (first 100 chars): ai_analyser_4eae4w6d/\n",
      "└── foo.txt\n",
      "\n",
      "2025-06-01 10:25:18,816 INFO [summary_synthesizer] _maybe_finish() called. analysis_done=True, tree_text_set=True\n",
      "2025-06-01 10:25:18,817 INFO [summary_synthesizer] Both prerequisites met, generating draft\n",
      "2025-06-01 10:25:18,817 DEBUG [summary_synthesizer] Draft generated (first 200 chars): The dominant file type is *text* (count: 1). Inferred tech stack: Unknown or mixed‐language project.\n",
      "2025-06-01 10:25:18,817 INFO [summary_synthesizer] Emitted ProjectDraft\n",
      "2025-06-01 10:25:18,817 INFO [summary_synthesizer] _polish() called with raw draft (first 200 chars): The dominant file type is *text* (count: 1). Inferred tech stack: Unknown or mixed‐language project.\n",
      "2025-06-01 10:25:22,951 WARNING [summary_synthesizer] LLM polish step failed: OPENAI_API_KEY env var missing\n",
      "2025-06-01 10:25:22,951 INFO [summary_synthesizer] Stored 'project_summary.txt' in memory (length=100)\n",
      "2025-06-01 10:25:22,952 INFO [summary_synthesizer] Emitted SummaryPolished with path 'project_summary.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummarySynthesizer emitted events:\n",
      "('ProjectDraft', {'draft': 'The dominant file type is *text* (count: 1). Inferred tech stack: Unknown or mixed‐language project.'})\n",
      "('SummaryPolished', {'summary_path': 'project_summary.txt'})\n",
      "\n",
      "Generated project_summary.txt:\n",
      "The dominant file type is *text* (count: 1). Inferred tech stack: Unknown or mixed‐language project.\n"
     ]
    }
   ],
   "source": [
    "from src.agents.summary_synthesizer_agent import SummarySynthesizerAgent\n",
    "\n",
    "# Prepare summary synthesizer agent\n",
    "summary_agent = SummarySynthesizerAgent()\n",
    "emitted_summary = []\n",
    "summary_agent.emit = lambda event_name, payload: emitted_summary.append((event_name, payload))\n",
    "\n",
    "# (1) Populate memory with the tree text from TreeBuilderAgent\n",
    "summary_agent.memory[\"project_tree.txt\"] = tree_agent.memory.get(\"project_tree.txt\", \"\")\n",
    "\n",
    "# (2) Feed in all FileAnalysed events (payloads) from file_analysis_agent\n",
    "analyses = [payload for (name, payload) in emitted_analysis if name == \"FileAnalysed\"]\n",
    "for analysis in analyses:\n",
    "    evt = {\"type\": \"FileAnalysed\", **analysis}\n",
    "    summary_agent.handle(evt)\n",
    "\n",
    "# (3) Signal that analysis is complete\n",
    "ac_event = {\"type\": \"AnalysisComplete\"}\n",
    "summary_agent.handle(ac_event)\n",
    "\n",
    "# (4) Also signal that the tree was built\n",
    "tb_event = {\"type\": \"TreeBuilt\", \"tree_path\": \"project_tree.txt\"}\n",
    "summary_agent.handle(tb_event)\n",
    "\n",
    "# (5) Inspect what got emitted\n",
    "print(\"SummarySynthesizer emitted events:\")\n",
    "for evt in emitted_summary:\n",
    "    print(evt)\n",
    "\n",
    "# (6) Check memory for the final project_summary.txt\n",
    "print(\"\\nGenerated project_summary.txt:\")\n",
    "print(summary_agent.memory.get(\"project_summary.txt\", \"<none>\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
