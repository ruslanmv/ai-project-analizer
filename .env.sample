# ===============================
# AI Project-Analizer — .env SAMPLE
# ===============================
# Copy this file to `.env` and fill in the values you need.
# Any variable left blank will fall back to a sensible default.

# ---------------------------------------------------------------------------
# 1) BeeAI LLM back-end
# ---------------------------------------------------------------------------
# Choose ONE model identifier.  Supported prefixes:
#   openai/…      – OpenAI ChatCompletion models   (needs OPENAI_API_KEY)
#   watsonx/…     – IBM watsonx.ai foundation models (needs WATSONX_API_KEY + WATSONX_URL)
#   ollama/…      – Local Ollama model name OR leave prefix off and just set OLLAMA_URL
#
# Examples:
#   openai/gpt-4o-mini
#   watsonx/meta-llama/llama-4-maverick-17b-128e-instruct-fp
#   ollama/llama3
#
BEEAI_MODEL=openai/gpt-4o-mini

# ---------------------------------------------------------------------------
# 2) Workflow housekeeping
# ---------------------------------------------------------------------------
# Delete the extracted temp directory after each run?  (true / false)
DELETE_TEMP_AFTER_RUN=true

# Maximum allowed upload size (compressed ZIP) in MB
ZIP_SIZE_LIMIT_MB=300

# Maximum allowed uncompressed size per ZIP member (in MB)
MAX_MEMBER_SIZE_MB=150

# ---------------------------------------------------------------------------
# 3) BeeAI memory store
# ---------------------------------------------------------------------------
# Default is a local SQLite file.  Override with postgres, mysql, etc.
BEEAI_MEMORY_DSN=sqlite:///state.db

# Log verbosity for BeeAI and the application (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# ---------------------------------------------------------------------------
# 4) FastAPI server
# ---------------------------------------------------------------------------
APP_HOST=0.0.0.0
APP_PORT=8000

# ---------------------------------------------------------------------------
# 5) Cloud LLM credentials
# ---------------------------------------------------------------------------
# OpenAI
OPENAI_API_KEY=

# IBM watsonx.ai
#WATSONX_API_KEY=
#WATSONX_URL=https://us-south.ml.cloud.ibm.com

# ---------------------------------------------------------------------------
# 6) Local Ollama settings (optional)
# ---------------------------------------------------------------------------
# If you run `docker run -p 11434:11434 ollama/ollama`, leave as default.
OLLAMA_URL=http://localhost:11434

# ---------------------------------------------------------------------------
# 7) Custom variables (extend as needed)
# ---------------------------------------------------------------------------
#CUSTOM_VAR_EXAMPLE=
