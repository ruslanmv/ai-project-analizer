# ===============================
# AI Project-Analyser Environment Sample
# ===============================

# 1) BeeAI LLM model override
#    - Specify which foundation model to use for LLM-based agents.
#    - Examples: "openai/gpt-4o-mini", "watsonx/gpt-5", or a local Ollama endpoint.
BEEAI_MODEL=

# 2) Toggle whether to delete the temp directory after each run.
#    - true  → remove extracted files after workflow completes
#    - false → keep the extraction folder around (useful for debugging)
DELETE_TEMP_AFTER_RUN=true

# 3) Maximum allowed size for the uploaded ZIP (in megabytes).
#    - Any upload larger than this will be rejected immediately.
ZIP_SIZE_LIMIT_MB=300

# 4) Maximum allowed size for a single uncompressed member (in megabytes).
#    - Prevents zip‐bombs or huge binaries from being extracted.
MAX_MEMBER_SIZE_MB=150

# 5) BeeAI memory store configuration
#    - Currently uses SQLite by default (state.db in working directory).
#    - You could override with a different DSN, e.g. PostgreSQL: "postgresql://user:pass@host/db"
BEEAI_MEMORY_DSN=sqlite:///state.db

# 6) Logging level
#    - Controls BeeAI and application log verbosity (DEBUG, INFO, WARNING, ERROR).
LOG_LEVEL=INFO

# 7) FastAPI / Uvicorn server settings
#    - PORT: which port the FastAPI app listens on.
#    - HOST: host binding (default "0.0.0.0" to allow external connections).
APP_HOST=0.0.0.0
APP_PORT=8000

# 8) Optional: OpenAI or IBM WatsonX API credentials (if using cloud LLMs)
#    - Set either OPENAI_API_KEY or WATSONX_API_KEY
OPENAI_API_KEY=
# WATSONX_API_KEY=
# WATSONX_URL=  # e.g. "https://api.us-south.watsonx.ai"

# 9) Ollama (local LLM) settings, if you run a local Ollama container
OLLAMA_URL=http://localhost:11434

# 10) Any additional environment variables your custom agents/tools might require
# CUSTOM_VAR_EXAMPLE=
