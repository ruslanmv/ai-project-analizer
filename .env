# ===============================
# AI Project-Analizer — .env
# ===============================
# Copy this file to `.env` and fill in the values you need.
# Variables not explicitly set here will fall back to sensible defaults.

# ---------------------------------------------------------------------------
# 1) BeeAI LLM back-end
# ---------------------------------------------------------------------------
# Choose ONE model identifier. Supported prefixes:
#   openai/…      – OpenAI ChatCompletion models   (requires OPENAI_API_KEY)
#   watsonx/…     – IBM watsonx.ai models          (requires WATSONX_API_KEY + WATSONX_URL)
#   ollama/…      – Local Ollama model name         (requires OLLAMA_URL)
#
# In this configuration, we will use the local Ollama model "granite3.1-dense:8b"
BEEAI_MODEL=ollama/granite3.1-dense:8b

# ---------------------------------------------------------------------------
# 2) Workflow housekeeping
# ---------------------------------------------------------------------------
# Delete the extracted temp directory after each run?  (true / false)
DELETE_TEMP_AFTER_RUN=true

# Maximum allowed upload size (compressed ZIP) in MB
ZIP_SIZE_LIMIT_MB=300

# Maximum allowed uncompressed size per ZIP member (in MB)
MAX_MEMBER_SIZE_MB=150

# ---------------------------------------------------------------------------
# 3) BeeAI memory store
# ---------------------------------------------------------------------------
# Default is a local SQLite file.
# To override (PostgreSQL, MySQL, etc.), set a different DSN, e.g.:
#   BEEAI_MEMORY_DSN=postgresql://user:pass@host/dbname
BEEAI_MEMORY_DSN=sqlite:///state.db

# Log verbosity for BeeAI and the application (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# ---------------------------------------------------------------------------
# 4) FastAPI server
# ---------------------------------------------------------------------------
# Host and port for the FastAPI application
APP_HOST=0.0.0.0
APP_PORT=8000

# ---------------------------------------------------------------------------
# 5) Cloud LLM credentials (optional, not needed for Ollama)
# ---------------------------------------------------------------------------
# If you switch to an OpenAI model:
# OPENAI_API_KEY=sk-...
OPENAI_API_KEY=

# If you switch to an IBM watsonx.ai model:
# WATSONX_API_KEY=
# WATSONX_URL=https://us-south.ml.cloud.ibm.com

# ---------------------------------------------------------------------------
# 6) Local Ollama settings (ensure Ollama is running on this endpoint)
# ---------------------------------------------------------------------------
# By default, Ollama listens on 11434 when run via:
#   docker run -p 11434:11434 ollama/ollama
OLLAMA_URL=http://localhost:11434

# ---------------------------------------------------------------------------
# 7) Custom variables (extend as needed)
# ---------------------------------------------------------------------------
# CUSTOM_VAR_EXAMPLE=
